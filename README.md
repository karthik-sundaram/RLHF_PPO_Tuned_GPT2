# RLHF_PPO_Tuned_GPT2
RLHF_PPO_Tuned_GPT2 fine-tunes GPT-2 using Reinforcement Learning from Human Feedback (RLHF) and Proximal Policy Optimization (PPO) to generate responses aligned with human preferences. This project enhances response helpfulness, truthfulness, and harmlessness through reward-based learning, significantly outperforming the vanilla GPT-2 model.
