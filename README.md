# RLHF_PPO_Tuned_GPT2
RLHF_PPO_Tuned_GPT2 fine-tunes GPT-2 using Reinforcement Learning from Human Feedback (RLHF) and Proximal Policy Optimization (PPO) to generate responses aligned with human preferences. This project enhances response helpfulness, truthfulness, and harmlessness through reward-based learning, significantly outperforming the vanilla GPT-2 model.

## Project Overview  

RLHF_PPO_Tuned_GPT2 enhances GPT-2's ability to generate helpful, truthful, and harmless responses using Reinforcement Learning from Human Feedback (RLHF) and Proximal Policy Optimization (PPO). By leveraging human feedback, this project fine-tunes the model for better response alignment and safety, optimizing behavior over time through reward-based learning.

Fine-Tuning Process
Reward Model Training:
A reward model was trained using human feedback from the yitingxie/rlhf-reward-datasets to assign positive or negative feedback based on response quality.
The dataset was filtered, limiting prompts and responses to under 500 tokens for efficiency during training.
PPO Tuning:
The model was further refined with PPO, ensuring responses are aligned with human feedback by optimizing behavior based on the reward scores generated by the reward model.
KL divergence and a conservative learning rate were used to maintain stability during training, improving response safety and relevance.
Model Architecture
Base Model: GPT-2 Medium.
Reward Model: Predicts feedback scores (positive or negative) based on the helpfulness and appropriateness of responses.
PPO Head: Enhances the GPT-2 model by introducing reinforcement learning to align model outputs with human feedback.
Inference and Evaluation
During inference, responses were generated using both the vanilla GPT-2 and the PPO-tuned GPT-2 models. Responses were compared using criteria like:

Helpfulness: Whether the response answered the question effectively.
Truthfulness: The factual accuracy of the response.
Harmlessness: Avoiding misleading or harmful content.
Output Analysis:
A total of 36 prompts were used for evaluation. The responses were reviewed and judged based on their adherence to the three criteria.

Results:

vanilla_responses: 9 better responses
ppo_responses: 23 better responses
Neither: 4 responses were equally poor.
The PPO-tuned GPT-2 model significantly outperformed the vanilla model, providing more relevant, less repetitive, and safer answers. PPO fine-tuning helped the model handle sensitive topics more responsibly, demonstrating greater care in avoiding harmful or misleading outputs.

Detailed Findings:
PPO-tuned responses were generally more coherent and aligned better with human feedback, making them more useful in conversational contexts.
Vanilla GPT-2 responses often lacked the depth and safety precautions observed in the PPO-tuned model, frequently generating repetitive or less informative answers.
Overall Summary
The RLHF PPO-tuned GPT-2 model offers significant improvements over the vanilla GPT-2, particularly in relevance, safety, and overall helpfulness. By combining RLHF and PPO, the model aligns more closely with human preferences, making it more suitable for real-world applications that require trustworthy and non-harmful outputs.
